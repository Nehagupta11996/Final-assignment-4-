{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution to Q number 1. Scrape the details of most viewed videos on YouTube from Wikipedia:\n",
    "#Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/\n",
    "#You need to find following details:\n",
    "#A) Rank\n",
    "#B) Name\n",
    "#C) Artist\n",
    "#D) Upload date\n",
    "#E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the link\n",
    "\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inside the link clicking on the other link\n",
    "time.sleep(5)\n",
    "button1=driver.find_element_by_xpath(\"//a[@href='https://en.wikipedia.org/w/index.php?search=List+of+most-viewed+YouTube+videos%2F&title=Special%3ASearch&fulltext=1&ns0=1']\").get_attribute('href')\n",
    "driver.get(button1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inside the link clicking on the other link\n",
    "time.sleep(5)\n",
    "button1=driver.find_element_by_xpath(\"//a[@href='/wiki/List_of_most-viewed_YouTube_videos']\").get_attribute('href')\n",
    "driver.get(button1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "#scrolling the page down\n",
    "driver.execute_script(\"window.scrollTo(0, 1300)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted the index data\n",
    "import re\n",
    "time.sleep(4)\n",
    "Index_No=[]\n",
    "index=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']/tbody/tr/td[1]\")\n",
    "for i in index:\n",
    "    if i.text is None :\n",
    "        Index_No.append(\"--\") \n",
    "    else:\n",
    "        Index_No.append(i.text)\n",
    "\n",
    "Index_No[:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted the name data\n",
    "\n",
    "time.sleep(4)\n",
    "Name=[]\n",
    "names=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']/tbody/tr/td[2]\")\n",
    "for i in names:\n",
    "    if i.text is None :\n",
    "        Name.append(\"--\") \n",
    "    else:\n",
    "        Name.append(i.text)\n",
    "\n",
    "Name[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted the artist data\n",
    "time.sleep(4)\n",
    "Artist=[]\n",
    "artists=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']/tbody/tr/td[2]/a\")\n",
    "for i in artists:\n",
    "    if i.text is None :\n",
    "        Artist.append(\"--\") \n",
    "    else:\n",
    "        Artist.append(i.text)\n",
    "\n",
    "Artist[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted the views data\n",
    "time.sleep(4)\n",
    "views=[]\n",
    "view=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']/tbody/tr/td[4]\")\n",
    "for i in view:\n",
    "    if i.text is None :\n",
    "        views.append(\"--\") \n",
    "    else:\n",
    "        views.append(i.text)\n",
    "\n",
    "views[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted the upload date value\n",
    "time.sleep(4)\n",
    "Upload_date=[]\n",
    "uploads=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']/tbody/tr/td[5]\")\n",
    "for i in uploads:\n",
    "    if i.text is None :\n",
    "        Upload_date.append(\"--\") \n",
    "    else:\n",
    "        Upload_date.append(i.text)\n",
    "\n",
    "Upload_date[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created the dataframe of all the values\n",
    "\n",
    "time.sleep(4)\n",
    "df1=pd.DataFrame({\" Rank\":Index_No[:30],\"Name\":Name[:30],\"Artistss\":Artist[:30],\"Viewss\":views[:30],\"Upload_dates\":Upload_date[:30]})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artistss</th>\n",
       "      <th>Viewss</th>\n",
       "      <th>Upload_dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[22]</td>\n",
       "      <td>Baby Shark Dance</td>\n",
       "      <td>8.96</td>\n",
       "      <td>June 17, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[24]</td>\n",
       "      <td>Despacito</td>\n",
       "      <td>7.44</td>\n",
       "      <td>January 12, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[25]</td>\n",
       "      <td>Johny Johny Yes Papa</td>\n",
       "      <td>5.55</td>\n",
       "      <td>October 8, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Shape of You\"[26]</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>5.38</td>\n",
       "      <td>January 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"See You Again\"[27]</td>\n",
       "      <td>See You Again</td>\n",
       "      <td>5.18</td>\n",
       "      <td>April 6, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[30]</td>\n",
       "      <td>Uptown Funk</td>\n",
       "      <td>4.44</td>\n",
       "      <td>January 31, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Bath Song\"[31]</td>\n",
       "      <td>Gangnam Style</td>\n",
       "      <td>4.31</td>\n",
       "      <td>May 2, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[32]</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>4.26</td>\n",
       "      <td>February 27, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Uptown Funk\"[33]</td>\n",
       "      <td>Dame Tu Cosita</td>\n",
       "      <td>4.23</td>\n",
       "      <td>November 19, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Gangnam Style\"[34]</td>\n",
       "      <td>Sorry</td>\n",
       "      <td>4.12</td>\n",
       "      <td>July 15, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[36]</td>\n",
       "      <td>Roar</td>\n",
       "      <td>3.99</td>\n",
       "      <td>March 6, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Sugar\"[37]</td>\n",
       "      <td>Counting Stars</td>\n",
       "      <td>3.51</td>\n",
       "      <td>January 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[38]</td>\n",
       "      <td>Thinking Out Loud</td>\n",
       "      <td>3.46</td>\n",
       "      <td>April 5, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Sorry\"[39]</td>\n",
       "      <td>Dark Horse</td>\n",
       "      <td>3.45</td>\n",
       "      <td>October 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Roar\"[40]</td>\n",
       "      <td>Faded</td>\n",
       "      <td>3.39</td>\n",
       "      <td>September 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Counting Stars\"[41]</td>\n",
       "      <td>Shake It Off</td>\n",
       "      <td>3.34</td>\n",
       "      <td>May 31, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Thinking Out Loud\"[42]</td>\n",
       "      <td>Girls Like You</td>\n",
       "      <td>3.29</td>\n",
       "      <td>October 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Wheels on the Bus\"[43]</td>\n",
       "      <td>Lean On</td>\n",
       "      <td>3.15</td>\n",
       "      <td>May 24, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Dark Horse\"[44]</td>\n",
       "      <td>Bailando</td>\n",
       "      <td>3.10</td>\n",
       "      <td>February 20, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Faded\"[45]</td>\n",
       "      <td>Let Her Go</td>\n",
       "      <td>3.10</td>\n",
       "      <td>December 3, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Shake It Off\"[46]</td>\n",
       "      <td>Mi Gente</td>\n",
       "      <td>3.07</td>\n",
       "      <td>August 18, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Girls Like You\"[47]</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>3.07</td>\n",
       "      <td>May 31, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Lean On\"[48]</td>\n",
       "      <td>Waka Waka (This Time for Africa)</td>\n",
       "      <td>3.06</td>\n",
       "      <td>March 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Bailando\"[49]</td>\n",
       "      <td>Axel F</td>\n",
       "      <td>3.06</td>\n",
       "      <td>April 11, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Let Her Go\"[50]</td>\n",
       "      <td>Hello</td>\n",
       "      <td>3.03</td>\n",
       "      <td>July 25, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Mi Gente\"[51]</td>\n",
       "      <td>Pinkfong</td>\n",
       "      <td>2.94</td>\n",
       "      <td>June 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Perfect\"[52]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>2.90</td>\n",
       "      <td>November 9, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[53]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>2.88</td>\n",
       "      <td>June 4, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Axel F\"[54]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>2.88</td>\n",
       "      <td>June 16, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Hello\"[55]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>2.85</td>\n",
       "      <td>October 22, 2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                             Name  \\\n",
       "0     1.                           \"Baby Shark Dance\"[22]   \n",
       "1     2.                                  \"Despacito\"[24]   \n",
       "2     3.                       \"Johny Johny Yes Papa\"[25]   \n",
       "3     4.                               \"Shape of You\"[26]   \n",
       "4     5.                              \"See You Again\"[27]   \n",
       "5     6.   \"Masha and the Bear – Recipe for Disaster\"[30]   \n",
       "6     7.                                  \"Bath Song\"[31]   \n",
       "7     8.  \"Learning Colors – Colorful Eggs on a Farm\"[32]   \n",
       "8     9.                                \"Uptown Funk\"[33]   \n",
       "9    10.                              \"Gangnam Style\"[34]   \n",
       "10   11.                \"Phonics Song with Two Words\"[36]   \n",
       "11   12.                                      \"Sugar\"[37]   \n",
       "12   13.                             \"Dame Tu Cosita\"[38]   \n",
       "13   14.                                      \"Sorry\"[39]   \n",
       "14   15.                                       \"Roar\"[40]   \n",
       "15   16.                             \"Counting Stars\"[41]   \n",
       "16   17.                          \"Thinking Out Loud\"[42]   \n",
       "17   18.                          \"Wheels on the Bus\"[43]   \n",
       "18   19.                                 \"Dark Horse\"[44]   \n",
       "19   20.                                      \"Faded\"[45]   \n",
       "20   21.                               \"Shake It Off\"[46]   \n",
       "21   22.                             \"Girls Like You\"[47]   \n",
       "22   23.                                    \"Lean On\"[48]   \n",
       "23   24.                                   \"Bailando\"[49]   \n",
       "24   25.                                 \"Let Her Go\"[50]   \n",
       "25   26.                                   \"Mi Gente\"[51]   \n",
       "26   27.                                    \"Perfect\"[52]   \n",
       "27   28.           \"Waka Waka (This Time for Africa)\"[53]   \n",
       "28   29.                                     \"Axel F\"[54]   \n",
       "29   30.                                      \"Hello\"[55]   \n",
       "\n",
       "                            Artistss Viewss       Upload_dates  \n",
       "0                   Baby Shark Dance   8.96      June 17, 2016  \n",
       "1                          Despacito   7.44   January 12, 2017  \n",
       "2               Johny Johny Yes Papa   5.55    October 8, 2016  \n",
       "3                       Shape of You   5.38   January 30, 2017  \n",
       "4                      See You Again   5.18      April 6, 2015  \n",
       "5                        Uptown Funk   4.44   January 31, 2012  \n",
       "6                      Gangnam Style   4.31        May 2, 2018  \n",
       "7                              Sugar   4.26  February 27, 2018  \n",
       "8                     Dame Tu Cosita   4.23  November 19, 2014  \n",
       "9                              Sorry   4.12      July 15, 2012  \n",
       "10                              Roar   3.99      March 6, 2014  \n",
       "11                    Counting Stars   3.51   January 14, 2015  \n",
       "12                 Thinking Out Loud   3.46      April 5, 2018  \n",
       "13                        Dark Horse   3.45   October 22, 2015  \n",
       "14                             Faded   3.39  September 5, 2013  \n",
       "15                      Shake It Off   3.34       May 31, 2013  \n",
       "16                    Girls Like You   3.29    October 7, 2014  \n",
       "17                           Lean On   3.15       May 24, 2018  \n",
       "18                          Bailando   3.10  February 20, 2014  \n",
       "19                        Let Her Go   3.10   December 3, 2015  \n",
       "20                          Mi Gente   3.07    August 18, 2014  \n",
       "21                           Perfect   3.07       May 31, 2018  \n",
       "22  Waka Waka (This Time for Africa)   3.06     March 22, 2015  \n",
       "23                            Axel F   3.06     April 11, 2014  \n",
       "24                             Hello   3.03      July 25, 2012  \n",
       "25                          Pinkfong   2.94      June 29, 2017  \n",
       "26                        Luis Fonsi   2.90   November 9, 2017  \n",
       "27                       Wiz Khalifa   2.88       June 4, 2010  \n",
       "28                               Psy   2.88      June 16, 2009  \n",
       "29                     Justin Bieber   2.85   October 22, 2015  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#opening the link\n",
    "\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)\n",
    "\n",
    "#inside the link clicking on the other link\n",
    "time.sleep(5)\n",
    "button1=driver.find_element_by_xpath(\"//a[@href='https://en.wikipedia.org/w/index.php?search=List+of+most-viewed+YouTube+videos%2F&title=Special%3ASearch&fulltext=1&ns0=1']\").get_attribute('href')\n",
    "driver.get(button1)\n",
    "\n",
    "#inside the link clicking on the other link\n",
    "time.sleep(5)\n",
    "button1=driver.find_element_by_xpath(\"//a[@href='/wiki/List_of_most-viewed_YouTube_videos']\").get_attribute('href')\n",
    "driver.get(button1)\n",
    "\n",
    "time.sleep(5)\n",
    "#scrolling the page down\n",
    "driver.execute_script(\"window.scrollTo(0, 1500)\") \n",
    "\n",
    "#extracted the index data\n",
    "time.sleep(4)\n",
    "Index_No=[]\n",
    "index=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']/tbody/tr/td[1]\")\n",
    "for i in index:\n",
    "    if i.text is None :\n",
    "        Index_No.append(\"--\") \n",
    "    else:\n",
    "        Index_No.append(i.text)\n",
    "\n",
    "Index_No[:30]\n",
    "\n",
    "#extracted the name data\n",
    "\n",
    "time.sleep(4)\n",
    "Name=[]\n",
    "names=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']/tbody/tr/td[2]\")\n",
    "for i in names:\n",
    "    if i.text is None :\n",
    "        Name.append(\"--\") \n",
    "    else:\n",
    "        Name.append(i.text)\n",
    "\n",
    "Name[:30]\n",
    "\n",
    "#extracted the artist data\n",
    "time.sleep(4)\n",
    "Artist=[]\n",
    "artists=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']/tbody/tr/td[2]/a\")\n",
    "for i in artists:\n",
    "    if i.text is None :\n",
    "        Artist.append(\"--\") \n",
    "    else:\n",
    "        Artist.append(i.text)\n",
    "\n",
    "Artist[:30]\n",
    "\n",
    "#extracted the views data\n",
    "time.sleep(4)\n",
    "views=[]\n",
    "view=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']/tbody/tr/td[4]\")\n",
    "for i in view:\n",
    "    if i.text is None :\n",
    "        views.append(\"--\") \n",
    "    else:\n",
    "        views.append(i.text)\n",
    "\n",
    "views[:30]\n",
    "\n",
    "#extracted the upload date value\n",
    "time.sleep(4)\n",
    "Upload_date=[]\n",
    "uploads=driver.find_elements_by_xpath(\"//table[@class='wikitable sortable jquery-tablesorter']/tbody/tr/td[5]\")\n",
    "for i in uploads:\n",
    "    if i.text is None :\n",
    "        Upload_date.append(\"--\") \n",
    "    else:\n",
    "        Upload_date.append(i.text)\n",
    "\n",
    "Upload_date[:30]\n",
    "\n",
    "#created the dataframe of all the values\n",
    "\n",
    "time.sleep(4)\n",
    "df1=pd.DataFrame({\" Rank\":Index_No[:30],\"Name\":Name[:30],\"Artistss\":Artist[:30],\"Viewss\":views[:30],\"Upload_dates\":Upload_date[:30]})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution to QNumber 2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "#Url = https://www.bcci.tv/.\n",
    "#You need to find following details:\n",
    "#A) Match title (I.e. 1st ODI)\n",
    "#B) Series\n",
    "#C) Place\n",
    "#D) Date\n",
    "#E) Time\n",
    "#Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the link\n",
    "\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://www.bcci.tv/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the international button\n",
    "time.sleep(6)\n",
    "search_button=driver.find_element_by_xpath('//div[@class=\"navigation__drop-down drop-down drop-down--reveal-on-hover\"]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the international fixtures button\n",
    "search_button=driver.find_element_by_xpath('//div[@class=\"drop-down__options\"]/ul/li')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R Premadasa Stadium, Colombo',\n",
       " 'R Premadasa Stadium, Colombo',\n",
       " 'R Premadasa Stadium, Colombo',\n",
       " 'R Premadasa Stadium, Colombo',\n",
       " 'R Premadasa Stadium, Colombo',\n",
       " 'R Premadasa Stadium, Colombo',\n",
       " 'Trent Bridge, Nottingham',\n",
       " \"Lord's, London\",\n",
       " 'Headingley, Leeds',\n",
       " 'The Oval, London',\n",
       " 'Old Trafford, Manchester']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracted the place name\n",
    "Place_Name=[]\n",
    "\n",
    "dt4 = driver.find_elements_by_xpath('//p[@class=\"fixture__additional-info\"]/span')\n",
    "\n",
    "for d in dt4:\n",
    "    Place_Name.append(d.text)\n",
    "\n",
    "Place_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15:00 IST',\n",
       " '15:00 IST',\n",
       " '15:00 IST',\n",
       " '20:00 IST',\n",
       " '20:00 IST',\n",
       " '20:00 IST',\n",
       " '15:30 IST',\n",
       " '15:30 IST',\n",
       " '15:30 IST',\n",
       " '15:30 IST',\n",
       " '15:30 IST']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracted the date\n",
    "Time=[]\n",
    "\n",
    "dt5 = driver.find_elements_by_xpath('//div[@class=\"js-list\"]/a/div/div/div/span[2]')\n",
    "\n",
    "for d in dt5:\n",
    "    Time.append(d.text)\n",
    "\n",
    "Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1st ODI',\n",
       " '2nd ODI',\n",
       " '3rd ODI',\n",
       " '1st T20I',\n",
       " '2nd T20I',\n",
       " '3rd T20I',\n",
       " '1st Test',\n",
       " '2nd Test',\n",
       " '3rd Test',\n",
       " '4th Test',\n",
       " '5th Test']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting the title name\n",
    "\n",
    "Title_Name=[]\n",
    "\n",
    "dt3 = driver.find_elements_by_xpath('//strong[@class=\"fixture__name fixture__name--with-margin\"]')\n",
    "\n",
    "for d in dt3:\n",
    "    Title_Name.append(d.text)\n",
    "\n",
    "Title_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JULY',\n",
       " 'JULY',\n",
       " 'JULY',\n",
       " 'JULY',\n",
       " 'JULY',\n",
       " 'JULY',\n",
       " 'AUGUST',\n",
       " 'AUGUST',\n",
       " 'AUGUST',\n",
       " 'SEPTEMBER',\n",
       " 'SEPTEMBER']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrapping half date data\n",
    "\n",
    "Date=[]\n",
    "\n",
    "dt6= driver.find_elements_by_xpath('//div[@class=\"js-list\"]/a/div/div/div/span[1]')\n",
    "\n",
    "for d in dt6:\n",
    "    Date.append(d.text)\n",
    "\n",
    "Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SRI LANKA',\n",
       " 'INDIA',\n",
       " 'SRI LANKA',\n",
       " 'INDIA',\n",
       " 'SRI LANKA',\n",
       " 'INDIA',\n",
       " 'SRI LANKA',\n",
       " 'INDIA',\n",
       " 'SRI LANKA',\n",
       " 'INDIA',\n",
       " 'SRI LANKA',\n",
       " 'INDIA',\n",
       " 'ENGLAND',\n",
       " 'INDIA',\n",
       " 'ENGLAND',\n",
       " 'INDIA',\n",
       " 'ENGLAND',\n",
       " 'INDIA',\n",
       " 'ENGLAND',\n",
       " 'INDIA',\n",
       " 'ENGLAND',\n",
       " 'INDIA']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrapping half date data\n",
    "\n",
    "Series=[]\n",
    "\n",
    "dt8= driver.find_elements_by_xpath('//div[@class=\"fixture__teams\"]/div/p')\n",
    "\n",
    "for d in dt8:\n",
    "    Series.append(d.text)\n",
    "\n",
    "Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Series[0]\n",
    "\n",
    "series=[]\n",
    "x=0\n",
    "for i in Series:\n",
    "    while x>0:\n",
    "        series.append(i[x]+i[x+1])\n",
    "        \n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title_Name</th>\n",
       "      <th>Time</th>\n",
       "      <th>Place_Name</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st ODI</td>\n",
       "      <td>15:00 IST</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>JULY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd ODI</td>\n",
       "      <td>15:00 IST</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>JULY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>15:00 IST</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>JULY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1st T20I</td>\n",
       "      <td>20:00 IST</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>JULY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>20:00 IST</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>JULY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Title_Name       Time                    Place_Name  Date\n",
       "0    1st ODI  15:00 IST  R Premadasa Stadium, Colombo  JULY\n",
       "1    2nd ODI  15:00 IST  R Premadasa Stadium, Colombo  JULY\n",
       "2    3rd ODI  15:00 IST  R Premadasa Stadium, Colombo  JULY\n",
       "3   1st T20I  20:00 IST  R Premadasa Stadium, Colombo  JULY\n",
       "4   2nd T20I  20:00 IST  R Premadasa Stadium, Colombo  JULY"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#created the dataframe of all the values\n",
    "\n",
    "time.sleep(4)\n",
    "dfs=pd.DataFrame({\"Title_Name\":Title_Name,\"Time\":Time,\"Place_Name\":Place_Name,\"Date\":Date})\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution to Q3. Scrape the details of selenium exception from guru99.com.\n",
    "#Url = https://www.guru99.com/\n",
    "#You need to find following details:\n",
    "#A) Name\n",
    "#B) Description\n",
    "#Note: - From guru99 home page you have to reach to selenium exception handling page through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the link\n",
    "\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://www.guru99.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = driver.find_element_by_xpath('//ul[@id=\"java_technologies\"]/li[3]/a').get_attribute('href')\n",
    "\n",
    "driver.get(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "#scrolling the page down\n",
    "driver.execute_script(\"window.scrollTo(0, 3500)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "button = driver.find_element_by_xpath('//a[@href=\"/exception-handling-selenium.html\"]').get_attribute('href')\n",
    "\n",
    "driver.get(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "#scrolling the page down\n",
    "driver.execute_script(\"window.scrollTo(0, 500)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ElementNotVisibleException This type of Selenium exception occurs when an existing element in DOM has a feature set as hidden.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracted the name data\n",
    "time.sleep(4)\n",
    "Name=[]\n",
    "Names=driver.find_elements_by_xpath(\"//table[@class='table table-striped']/tbody/tr[2]/td[i]\")\n",
    "for i in Names:\n",
    "    if i.text is None :\n",
    "        Name.append(\"--\") \n",
    "    else:\n",
    "        Name.append(i.text)\n",
    "\n",
    "Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This type of Selenium exception occurs when an existing element in DOM has a feature set as hidden.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracted the description\n",
    "time.sleep(4)\n",
    "Description=[]\n",
    "desc=driver.find_elements_by_xpath(\"//table[@class='table table-striped']/tbody/tr[2]/td[2]\")\n",
    "for i in desc:\n",
    "    if i.text is None :\n",
    "        Description.append(\"--\") \n",
    "    else:\n",
    "        Description.append(i.text)\n",
    "\n",
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution to Q4. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "#Url = http://statisticstimes.com/\n",
    "#You have to find following details:\n",
    "#A) Rank\n",
    "#B) State\n",
    "#C) GSDP at current price (19-20)\n",
    "#D) GSDP at current price (18-19)\n",
    "#E) Share(18-19)\n",
    "#F) GDP($ billion)\n",
    "#Note: - From statisticstimes home page you have to reach to economy page through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# Opening the statisticstimes.com\n",
    "url = \"http://statisticstimes.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(6)\n",
    "search_button=driver.find_element_by_xpath('//div[@class=\"navbar\"]/div[2]/button')\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "button = driver.find_element_by_xpath('//div[@class=\"navbar\"]/div[2]/div/a[3]').get_attribute('href')\n",
    "\n",
    "driver.get(button)\n",
    "\n",
    "#clicking on gdp of indian states\n",
    "time.sleep(5)\n",
    "\n",
    "button = driver.find_element_by_xpath('//ul[@style=\"list-style-type:none;margin-left:20px;\"]/li/a').get_attribute('href')\n",
    "\n",
    "driver.get(button)\n",
    "\n",
    "time.sleep(30)\n",
    "#scrolling the page down\n",
    "driver.execute_script(\"window.scrollTo(0, 1500)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State=[]\n",
    "GSDP_1920=[]\n",
    "GSDP_1819=[]\n",
    "Share_1819=[]\n",
    "GDP_billion=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping rank\n",
    "rank=[]\n",
    "try:\n",
    "    ranks=driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']/table/tbody/tr/td[1]\")\n",
    "    for i in ranks:\n",
    "        rank.append(i.text)\n",
    "except NoSuchElementException:                 #handling no such element exception\n",
    "    rank.append('No details available')\n",
    "time.sleep(2)\n",
    "\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Maharashtra',\n",
       " 'Tamil Nadu',\n",
       " 'Uttar Pradesh',\n",
       " 'Gujarat',\n",
       " 'Karnataka',\n",
       " 'West Bengal',\n",
       " 'Rajasthan',\n",
       " 'Andhra Pradesh',\n",
       " 'Telangana',\n",
       " 'Madhya Pradesh',\n",
       " 'Kerala',\n",
       " 'Delhi',\n",
       " 'Haryana',\n",
       " 'Bihar',\n",
       " 'Punjab',\n",
       " 'Odisha',\n",
       " 'Assam',\n",
       " 'Chhattisgarh',\n",
       " 'Jharkhand',\n",
       " 'Uttarakhand',\n",
       " 'Jammu & Kashmir',\n",
       " 'Himachal Pradesh',\n",
       " 'Goa',\n",
       " 'Tripura',\n",
       " 'Chandigarh',\n",
       " 'Puducherry',\n",
       " 'Meghalaya',\n",
       " 'Sikkim',\n",
       " 'Manipur',\n",
       " 'Nagaland',\n",
       " 'Arunachal Pradesh',\n",
       " 'Mizoram',\n",
       " 'Andaman & Nicobar Islands']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping state\n",
    "State=[]\n",
    "try:\n",
    "    state=driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']/table/tbody/tr/td[2]\")\n",
    "    for i in state:\n",
    "        State.append(i.text)\n",
    "except NoSuchElementException:                 #handling no such element exception\n",
    "    State.append('No details available')\n",
    "time.sleep(2)\n",
    "\n",
    "State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " '1,845,853',\n",
       " '1,687,818',\n",
       " '-',\n",
       " '1,631,977',\n",
       " '1,253,832',\n",
       " '1,020,989',\n",
       " '972,782',\n",
       " '969,604',\n",
       " '906,672',\n",
       " '-',\n",
       " '856,112',\n",
       " '831,610',\n",
       " '611,804',\n",
       " '574,760',\n",
       " '521,275',\n",
       " '-',\n",
       " '329,180',\n",
       " '328,598',\n",
       " '-',\n",
       " '-',\n",
       " '165,472',\n",
       " '80,449',\n",
       " '55,984',\n",
       " '-',\n",
       " '38,253',\n",
       " '36,572',\n",
       " '32,496',\n",
       " '31,790',\n",
       " '-',\n",
       " '-',\n",
       " '26,503',\n",
       " '-']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping GSDP_1920\n",
    "GSDP_1920=[]\n",
    "try:\n",
    "    gsdp=driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']/table/tbody/tr/td[3]\")\n",
    "    for i in gsdp:\n",
    "        GSDP_1920.append(i.text)\n",
    "except NoSuchElementException:                 #handling no such element exception\n",
    "    GSDP_1920.append('No details available')\n",
    "time.sleep(2)\n",
    "\n",
    "GSDP_1920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2,632,792',\n",
       " '1,630,208',\n",
       " '1,584,764',\n",
       " '1,502,899',\n",
       " '1,493,127',\n",
       " '1,089,898',\n",
       " '942,586',\n",
       " '862,957',\n",
       " '861,031',\n",
       " '809,592',\n",
       " '781,653',\n",
       " '774,870',\n",
       " '734,163',\n",
       " '530,363',\n",
       " '526,376',\n",
       " '487,805',\n",
       " '315,881',\n",
       " '304,063',\n",
       " '297,204',\n",
       " '245,895',\n",
       " '155,956',\n",
       " '153,845',\n",
       " '73,170',\n",
       " '49,845',\n",
       " '42,114',\n",
       " '34,433',\n",
       " '33,481',\n",
       " '28,723',\n",
       " '27,870',\n",
       " '27,283',\n",
       " '24,603',\n",
       " '22,287',\n",
       " '-']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping GSDP_1819\n",
    "GSDP_1819=[]\n",
    "try:\n",
    "    gsdp1819=driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']/table/tbody/tr/td[4]\")\n",
    "    for i in gsdp1819:\n",
    "        GSDP_1819.append(i.text)\n",
    "except NoSuchElementException:                 #handling no such element exception\n",
    "    GSDP_1819.append('No details available')\n",
    "time.sleep(2)\n",
    "\n",
    "GSDP_1819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13.94%',\n",
       " '8.63%',\n",
       " '8.39%',\n",
       " '7.96%',\n",
       " '7.91%',\n",
       " '5.77%',\n",
       " '4.99%',\n",
       " '4.57%',\n",
       " '4.56%',\n",
       " '4.29%',\n",
       " '4.14%',\n",
       " '4.10%',\n",
       " '3.89%',\n",
       " '2.81%',\n",
       " '2.79%',\n",
       " '2.58%',\n",
       " '1.67%',\n",
       " '1.61%',\n",
       " '1.57%',\n",
       " '1.30%',\n",
       " '0.83%',\n",
       " '0.81%',\n",
       " '0.39%',\n",
       " '0.26%',\n",
       " '0.22%',\n",
       " '0.18%',\n",
       " '0.18%',\n",
       " '0.15%',\n",
       " '0.15%',\n",
       " '0.14%',\n",
       " '0.13%',\n",
       " '0.12%',\n",
       " '-']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping Share_1819\n",
    "Share_1819=[]\n",
    "try:\n",
    "    share=driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']/table/tbody/tr/td[5]\")\n",
    "    for i in share:\n",
    "        Share_1819.append(i.text)\n",
    "except NoSuchElementException:                 #handling no such element exception\n",
    "    Share_1819.append('No details available')\n",
    "time.sleep(2)\n",
    "\n",
    "Share_1819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['399.921',\n",
       " '247.629',\n",
       " '240.726',\n",
       " '228.290',\n",
       " '226.806',\n",
       " '165.556',\n",
       " '143.179',\n",
       " '131.083',\n",
       " '130.791',\n",
       " '122.977',\n",
       " '118.733',\n",
       " '117.703',\n",
       " '111.519',\n",
       " '80.562',\n",
       " '79.957',\n",
       " '74.098',\n",
       " '47.982',\n",
       " '46.187',\n",
       " '45.145',\n",
       " '37.351',\n",
       " '23.690',\n",
       " '23.369',\n",
       " '11.115',\n",
       " '7.571',\n",
       " '6.397',\n",
       " '5.230',\n",
       " '5.086',\n",
       " '4.363',\n",
       " '4.233',\n",
       " '4.144',\n",
       " '3.737',\n",
       " '3.385',\n",
       " '-']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping GDP_billion\n",
    "GDP_billion=[]\n",
    "try:\n",
    "    gdp=driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']/table/tbody/tr/td[6]\")\n",
    "    for i in gdp:\n",
    "        GDP_billion.append(i.text)\n",
    "except NoSuchElementException:                 #handling no such element exception\n",
    "    GDP_billion.append('No details available')\n",
    "time.sleep(2)\n",
    "\n",
    "GDP_billion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>States</th>\n",
       "      <th>GSDP__1920</th>\n",
       "      <th>GSDP__1819</th>\n",
       "      <th>Shares_1819</th>\n",
       "      <th>GDP_billions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank         States GSDP__1920 GSDP__1819 Shares_1819 GDP_billions\n",
       "0     1    Maharashtra          -  2,632,792      13.94%      399.921\n",
       "1     2     Tamil Nadu  1,845,853  1,630,208       8.63%      247.629\n",
       "2     3  Uttar Pradesh  1,687,818  1,584,764       8.39%      240.726\n",
       "3     4        Gujarat          -  1,502,899       7.96%      228.290\n",
       "4     5      Karnataka  1,631,977  1,493,127       7.91%      226.806"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#created the dataframe of all the values\n",
    "\n",
    "time.sleep(4)\n",
    "df=pd.DataFrame({\" Rank\":rank,\"States\":State,\"GSDP__1920\":GSDP_1920,\"GSDP__1819\":GSDP_1819,\"Shares_1819\":Share_1819,\"GDP_billions\":GDP_billion})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution to Q.5. Scrape the details of trending repositories on Github.com.\n",
    "#Url = https://github.com/\n",
    "#You have to find the following details:\n",
    "#A) Repository title\n",
    "#B) Repository description\n",
    "#C) Contributors count\n",
    "#D) Language usedASSIGNMENT\n",
    "#Note: - From the home page you have to click on the trending option from Explore menu through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the link\n",
    "\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://github.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hover on explore button\n",
    "time.sleep(5)\n",
    "button1=driver.find_element_by_xpath(\"//ul[@class='d-lg-flex list-style-none']/li[4]\")\n",
    "button1.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on the trending button \n",
    "\n",
    "time.sleep(5)\n",
    "button1=driver.find_element_by_xpath(\"//a[@href='/trending']\").get_attribute('href')\n",
    "driver.get(button1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data to be stored in this empty lists\n",
    "\n",
    "Repository_title=[]\n",
    "\n",
    "Language_Used=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TheAlgorithms / Python',\n",
       " 'paritytech / substrate',\n",
       " 'discordjs / discord.js',\n",
       " 'github / docs',\n",
       " 'livekit / livekit-server',\n",
       " 'ManimCommunity / manim',\n",
       " 'snowpackjs / astro',\n",
       " 'deepmind / alphafold',\n",
       " 'facebookresearch / ParlAI',\n",
       " 'GPUOpen-Effects / FidelityFX-FSR',\n",
       " 'GokuMohandas / MadeWithML',\n",
       " 'coolsnowwolf / lede',\n",
       " 'hit-thusz-RookieCJ / CSYuTuiMian2021',\n",
       " 'jonasschmedtmann / html-css-course',\n",
       " 'RosettaCommons / RoseTTAFold',\n",
       " 'lxgw / LxgwWenKai',\n",
       " 'bradtraversy / hulu-webpage-clone',\n",
       " 'ventoy / Ventoy',\n",
       " 'vuejs / vue-next',\n",
       " 'CaffeineMC / sodium-fabric',\n",
       " 'megaease / easegress',\n",
       " 'adiwajshing / Baileys',\n",
       " 'awesome-selfhosted / awesome-selfhosted',\n",
       " '3b1b / manim',\n",
       " 'public-apis / public-apis']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracted the Repository_title\n",
    "time.sleep(4)\n",
    "titles=driver.find_elements_by_xpath(\"//h1[@class='h3 lh-condensed']/a\")\n",
    "for i in titles:\n",
    "    if i.text is None :\n",
    "        Repository_title.append(\"--\") \n",
    "    else:\n",
    "        Repository_title.append(i.text)\n",
    "\n",
    "Repository_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Repository_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted the Repository_description\n",
    "# making empty lists    \n",
    "\n",
    "repo_urls=[]\n",
    "\n",
    "repo_desc=[]\n",
    "\n",
    "\n",
    "\n",
    "#scraping repositories urls\n",
    "\n",
    "repos=driver.find_elements_by_xpath(\"//h1[@class='h3 lh-condensed']/a\")\n",
    "\n",
    "for i in repos:\n",
    "\n",
    "    repo_urls.append(i.get_attribute('href'))\n",
    "\n",
    "time.sleep(2) \n",
    "\n",
    "repo_desc = []\n",
    "\n",
    "for i in repo_urls:\n",
    "\n",
    "    driver.get(i)\n",
    "\n",
    "     #scraping repositories discription\n",
    "\n",
    "    try:\n",
    "\n",
    "        repo = driver.find_element_by_xpath(\"/html/body/div[4]/div/main/div[2]/div/div/div[2]/div[2]/div/div[1]/div/p\")\n",
    "\n",
    "        repo_desc.append(repo.text)\n",
    "\n",
    "    except NoSuchElementException:\n",
    "\n",
    "        repo_desc.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All Algorithms implemented in Python',\n",
       " 'Substrate: The platform for blockchain innovators',\n",
       " 'A powerful JavaScript library for interacting with the Discord API',\n",
       " 'The open-source repo for docs.github.com',\n",
       " 'Distributed audio/video rooms over WebRTC',\n",
       " 'A community-maintained Python framework for creating mathematical animations.',\n",
       " '🚀🧑\\u200d🚀 Keep your eyes to the skies, astronauts!',\n",
       " 'Open source code for AlphaFold.',\n",
       " 'A framework for training and evaluating AI models on a variety of openly available dialogue datasets.',\n",
       " 'FidelityFX Super Resolution',\n",
       " 'Learn how to responsibly deliver value with ML.',\n",
       " \"Lean's OpenWrt source\",\n",
       " '关于2021年CS保研预推免通知公告的汇总',\n",
       " 'Starter files, final projects, and FAQ for my HTML + CSS course',\n",
       " 'This package contains deep learning models and related scripts for RoseTTAFold',\n",
       " \"An open-source Chinese font derived from Fontworks' Klee One. 一款基于 FONTWORKS 的 Klee One 的开源中文字体。\",\n",
       " 'Hulu webpage clone',\n",
       " 'A new bootable USB solution.',\n",
       " '🖖 Vue.js is a progressive, incrementally-adoptable JavaScript framework for building UI on the web.',\n",
       " 'A Fabric mod designed to improve frame rates and reduce micro-stutter',\n",
       " 'A Cloud Native traffic orchestration system',\n",
       " 'Lightweight full-featured typescript/javascript WhatsApp Web API',\n",
       " 'A list of Free Software network services and web applications which can be hosted on your own servers',\n",
       " 'Animation engine for explanatory math videos',\n",
       " 'A collective list of free APIs']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hover on explore button\n",
    "time.sleep(5)\n",
    "button1=driver.find_element_by_xpath(\"//ul[@class='d-lg-flex list-style-none']/li[4]\")\n",
    "button1.click()\n",
    "\n",
    "#clicking on the trending button \n",
    "\n",
    "time.sleep(5)\n",
    "button1=driver.find_element_by_xpath(\"//a[@href='/trending']\").get_attribute('href')\n",
    "driver.get(button1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted the contributors count\n",
    "# making empty lists    \n",
    "\n",
    "Contributors_count=[]\n",
    "\n",
    "for i in repo_urls:\n",
    "\n",
    "    driver.get(i)\n",
    "\n",
    "     #scraping contributors count\n",
    "\n",
    "    try:\n",
    "\n",
    "        contribution = driver.find_element_by_xpath(\"/html/body/div[4]/div/main/div[2]/div/div/div[2]/div[2]/div/div[4]/div/div/h2/a\")\n",
    "\n",
    "        Contributors_count.append(contribution.text)\n",
    "\n",
    "    except NoSuchElementException:\n",
    "\n",
    "        Contributors_count.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Contributors_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracted the language used\n",
    "time.sleep(4)\n",
    "language=driver.find_elements_by_xpath(\"//span[@class='repo-language-color']/span\")\n",
    "for i in language:\n",
    "    if i.text is None :\n",
    "        Language_Used.append(\"--\") \n",
    "    else:\n",
    "        Language_Used.append(i.text)\n",
    "\n",
    "Language_Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Repository_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All Algorithms implemented in Python</td>\n",
       "      <td>TheAlgorithms / Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Substrate: The platform for blockchain innovators</td>\n",
       "      <td>paritytech / substrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A powerful JavaScript library for interacting ...</td>\n",
       "      <td>discordjs / discord.js</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The open-source repo for docs.github.com</td>\n",
       "      <td>github / docs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Distributed audio/video rooms over WebRTC</td>\n",
       "      <td>livekit / livekit-server</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A community-maintained Python framework for cr...</td>\n",
       "      <td>ManimCommunity / manim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>🚀🧑‍🚀 Keep your eyes to the skies, astronauts!</td>\n",
       "      <td>snowpackjs / astro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Open source code for AlphaFold.</td>\n",
       "      <td>deepmind / alphafold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A framework for training and evaluating AI mod...</td>\n",
       "      <td>facebookresearch / ParlAI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FidelityFX Super Resolution</td>\n",
       "      <td>GPUOpen-Effects / FidelityFX-FSR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Learn how to responsibly deliver value with ML.</td>\n",
       "      <td>GokuMohandas / MadeWithML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lean's OpenWrt source</td>\n",
       "      <td>coolsnowwolf / lede</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>关于2021年CS保研预推免通知公告的汇总</td>\n",
       "      <td>hit-thusz-RookieCJ / CSYuTuiMian2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Starter files, final projects, and FAQ for my ...</td>\n",
       "      <td>jonasschmedtmann / html-css-course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This package contains deep learning models and...</td>\n",
       "      <td>RosettaCommons / RoseTTAFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>An open-source Chinese font derived from Fontw...</td>\n",
       "      <td>lxgw / LxgwWenKai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hulu webpage clone</td>\n",
       "      <td>bradtraversy / hulu-webpage-clone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A new bootable USB solution.</td>\n",
       "      <td>ventoy / Ventoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>🖖 Vue.js is a progressive, incrementally-adopt...</td>\n",
       "      <td>vuejs / vue-next</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A Fabric mod designed to improve frame rates a...</td>\n",
       "      <td>CaffeineMC / sodium-fabric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A Cloud Native traffic orchestration system</td>\n",
       "      <td>megaease / easegress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Lightweight full-featured typescript/javascrip...</td>\n",
       "      <td>adiwajshing / Baileys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>A list of Free Software network services and w...</td>\n",
       "      <td>awesome-selfhosted / awesome-selfhosted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Animation engine for explanatory math videos</td>\n",
       "      <td>3b1b / manim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A collective list of free APIs</td>\n",
       "      <td>public-apis / public-apis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Description  \\\n",
       "0                All Algorithms implemented in Python   \n",
       "1   Substrate: The platform for blockchain innovators   \n",
       "2   A powerful JavaScript library for interacting ...   \n",
       "3            The open-source repo for docs.github.com   \n",
       "4           Distributed audio/video rooms over WebRTC   \n",
       "5   A community-maintained Python framework for cr...   \n",
       "6       🚀🧑‍🚀 Keep your eyes to the skies, astronauts!   \n",
       "7                     Open source code for AlphaFold.   \n",
       "8   A framework for training and evaluating AI mod...   \n",
       "9                         FidelityFX Super Resolution   \n",
       "10    Learn how to responsibly deliver value with ML.   \n",
       "11                              Lean's OpenWrt source   \n",
       "12                              关于2021年CS保研预推免通知公告的汇总   \n",
       "13  Starter files, final projects, and FAQ for my ...   \n",
       "14  This package contains deep learning models and...   \n",
       "15  An open-source Chinese font derived from Fontw...   \n",
       "16                                 Hulu webpage clone   \n",
       "17                       A new bootable USB solution.   \n",
       "18  🖖 Vue.js is a progressive, incrementally-adopt...   \n",
       "19  A Fabric mod designed to improve frame rates a...   \n",
       "20        A Cloud Native traffic orchestration system   \n",
       "21  Lightweight full-featured typescript/javascrip...   \n",
       "22  A list of Free Software network services and w...   \n",
       "23       Animation engine for explanatory math videos   \n",
       "24                     A collective list of free APIs   \n",
       "\n",
       "                           Repository_title  \n",
       "0                    TheAlgorithms / Python  \n",
       "1                    paritytech / substrate  \n",
       "2                    discordjs / discord.js  \n",
       "3                             github / docs  \n",
       "4                  livekit / livekit-server  \n",
       "5                    ManimCommunity / manim  \n",
       "6                        snowpackjs / astro  \n",
       "7                      deepmind / alphafold  \n",
       "8                 facebookresearch / ParlAI  \n",
       "9          GPUOpen-Effects / FidelityFX-FSR  \n",
       "10                GokuMohandas / MadeWithML  \n",
       "11                      coolsnowwolf / lede  \n",
       "12     hit-thusz-RookieCJ / CSYuTuiMian2021  \n",
       "13       jonasschmedtmann / html-css-course  \n",
       "14             RosettaCommons / RoseTTAFold  \n",
       "15                        lxgw / LxgwWenKai  \n",
       "16        bradtraversy / hulu-webpage-clone  \n",
       "17                          ventoy / Ventoy  \n",
       "18                         vuejs / vue-next  \n",
       "19               CaffeineMC / sodium-fabric  \n",
       "20                     megaease / easegress  \n",
       "21                    adiwajshing / Baileys  \n",
       "22  awesome-selfhosted / awesome-selfhosted  \n",
       "23                             3b1b / manim  \n",
       "24                public-apis / public-apis  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#created the dataframe of all the values\n",
    "\n",
    "time.sleep(4)\n",
    "df5=pd.DataFrame({\"Description\":repo_desc,\"Repository_title\":Repository_title})\n",
    "df5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution to Q.6 Scrape the details of top 100 songs on billboard.com.\n",
    "#Url = https://www.billboard.com/\n",
    "#You have to find the following details:\n",
    "#A) Song name\n",
    "#B) Artist name\n",
    "#C) Last week rank\n",
    "#D) Peak rank\n",
    "#E) Weeks on board\n",
    "#Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the link\n",
    "\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://www.billboard.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hover on charts button\n",
    "time.sleep(5)\n",
    "button1=driver.find_element_by_xpath(\"//a[@href='/charts']\").get_attribute('href')\n",
    "driver.get(button1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on the top 100\n",
    "\n",
    "time.sleep(20)\n",
    "button1=driver.find_element_by_xpath(\"//a[@href='/charts/hot-100']\").get_attribute('href')\n",
    "driver.get(button1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "#scrolling the page down\n",
    "driver.execute_script(\"window.scrollTo(0, 1000)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the empty folders\n",
    "Song_name=[]\n",
    "Artist_name=[]\n",
    "Last_week_rank=[]\n",
    "Peak_rank=[]\n",
    "Weeks_board=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Butter',\n",
       " 'Good 4 U',\n",
       " 'Levitating',\n",
       " 'Kiss Me More',\n",
       " 'Montero (Call Me By Your Name)',\n",
       " 'Bad Habits',\n",
       " 'Leave The Door Open',\n",
       " 'Peaches',\n",
       " 'Save Your Tears',\n",
       " 'Deja Vu',\n",
       " 'Astronaut In The Ocean',\n",
       " 'Rapstar',\n",
       " 'You Right',\n",
       " 'Am I The Only One',\n",
       " 'Blinding Lights',\n",
       " 'Without You',\n",
       " 'Thot Shit',\n",
       " 'Forever After All',\n",
       " 'Heartbreak Anniversary',\n",
       " 'Fancy Like',\n",
       " 'Every Chance I Get',\n",
       " 'Famous Friends',\n",
       " 'Beautiful Mistakes',\n",
       " 'Best Friend',\n",
       " 'Lil Bit',\n",
       " 'Single Saturday Night',\n",
       " 'Heat Waves',\n",
       " 'Leave Before You Love Me',\n",
       " 'Wants And Needs',\n",
       " 'Yonaguni',\n",
       " 'Blame It On You',\n",
       " 'Late At Night',\n",
       " 'Drivers License',\n",
       " 'Telepatia',\n",
       " 'Glad You Exist',\n",
       " 'Mood',\n",
       " 'Wockesha',\n",
       " 'Traitor',\n",
       " 'Todo de Ti',\n",
       " \"Drinkin' Beer. Talkin' God. Amen.\",\n",
       " \"Ain't Shit\",\n",
       " 'pov',\n",
       " \"My Ex's Best Friend\",\n",
       " 'WUSYANAME',\n",
       " 'Calling My Phone',\n",
       " \"Beggin'\",\n",
       " 'Ball If I Want To',\n",
       " 'Gone',\n",
       " 'Wasting Time',\n",
       " 'Red Light Green Light',\n",
       " 'Things A Man Oughta Know',\n",
       " 'Favorite Crime',\n",
       " 'Settling Down',\n",
       " 'Way Less Sad',\n",
       " 'Straightenin',\n",
       " 'Ski',\n",
       " 'AM',\n",
       " 'Arcade',\n",
       " 'Happier',\n",
       " 'You',\n",
       " 'my.life',\n",
       " 'Chasing After You',\n",
       " \"Drunk (And I Don't Wanna Go Home)\",\n",
       " 'Need To Know',\n",
       " 'Track Star',\n",
       " 'One Too Many',\n",
       " 'Hats Off',\n",
       " 'Brutal',\n",
       " 'Build A Bitch',\n",
       " \"Breaking Up Was Easy In The 90's\",\n",
       " 'Country Again',\n",
       " 'Fiel',\n",
       " 'Renegade',\n",
       " 'Waves',\n",
       " \"We Didn't Have Much\",\n",
       " 'Minimum Wage',\n",
       " 'Follow You',\n",
       " 'Jealousy, Jealousy',\n",
       " 'Come Through',\n",
       " 'Twerkulator',\n",
       " 'Cry No More',\n",
       " 'Essence',\n",
       " 'A-O-K',\n",
       " '4 Da Gang',\n",
       " 'Tombstone',\n",
       " 'Made For You',\n",
       " 'Having Our Way',\n",
       " 'My Boy',\n",
       " 'transparentsoul',\n",
       " 'You Should Probably Leave',\n",
       " 'I Was On A Boat That Day',\n",
       " 'Outside',\n",
       " 'Working',\n",
       " 'Miss The Rage',\n",
       " 'Cold Beer Calling My Name',\n",
       " 'All I Know So Far',\n",
       " \"What's Next\",\n",
       " 'Enough For You',\n",
       " 'Juggernaut',\n",
       " 'Tell Em']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting the song name\n",
    "\n",
    "dt = driver.find_elements_by_xpath('//span[@class=\"chart-element__information__song text--truncate color--primary\"]')\n",
    "\n",
    "for d in dt:\n",
    "    Song_name.append(d.text)\n",
    "        \n",
    "Song_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Song_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracted the artist name\n",
    "try:\n",
    "\n",
    "    ranks=driver.find_elements_by_xpath('//span[@class=\"chart-element__information__artist text--truncate color--secondary\"]')\n",
    "\n",
    "    for i in ranks:\n",
    "\n",
    "        Artist_name.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Artist_name.append('No details available') \n",
    "\n",
    "Artist_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Artist_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracted the last week rank\n",
    "try:\n",
    "\n",
    "    ranks=driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--last']\")\n",
    "\n",
    "    for i in ranks:\n",
    "\n",
    "        Last_week_rank.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Last_week_rank.append('No details available') \n",
    "\n",
    "len(Last_week_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracted the ranks\n",
    "try:\n",
    "\n",
    "    ranks=driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--peak']\")\n",
    "\n",
    "    for i in ranks:\n",
    "\n",
    "        Peak_rank.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Peak_rank.append('No details available') \n",
    "\n",
    "len(Peak_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracted the weeks on chart\n",
    "try:\n",
    "\n",
    "    ranks=driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--week']\")\n",
    "\n",
    "    for i in ranks:\n",
    "\n",
    "        Weeks_board.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Weeks_board.append('No details available') \n",
    "\n",
    "len(Weeks_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Song_name),len(Artist_name),len(Last_week_rank),len(Peak_rank),len(Weeks_board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Songs_name</th>\n",
       "      <th>Artist_names</th>\n",
       "      <th>Last_week_ranks</th>\n",
       "      <th>Peak_ranks</th>\n",
       "      <th>Weeks_boards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Butter</td>\n",
       "      <td>BTS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good 4 U</td>\n",
       "      <td>Olivia Rodrigo</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Levitating</td>\n",
       "      <td>Dua Lipa Featuring DaBaby</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kiss Me More</td>\n",
       "      <td>Doja Cat Featuring SZA</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Montero (Call Me By Your Name)</td>\n",
       "      <td>Lil Nas X</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Songs_name               Artist_names Last_week_ranks  \\\n",
       "0                          Butter                        BTS               1   \n",
       "1                        Good 4 U             Olivia Rodrigo               2   \n",
       "2                      Levitating  Dua Lipa Featuring DaBaby               4   \n",
       "3                    Kiss Me More     Doja Cat Featuring SZA               3   \n",
       "4  Montero (Call Me By Your Name)                  Lil Nas X               8   \n",
       "\n",
       "  Peak_ranks Weeks_boards  \n",
       "0          1            7  \n",
       "1          1            8  \n",
       "2          2           40  \n",
       "3          3           13  \n",
       "4          1           15  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataframe created\n",
    "time.sleep(4)\n",
    "df2=pd.DataFrame({\"Songs_name\":Song_name,\"Artist_names\":Artist_name,\"Last_week_ranks\":Last_week_rank,\"Peak_ranks\":Peak_rank,\"Weeks_boards\":Weeks_board})\n",
    "df2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question Number 7.Scrape the details of Data science recruiters from naukri.com.\n",
    "#Url = https://www.naukri.com/\n",
    "#You have to find the following details:\n",
    "#A) Name\n",
    "#B) Designation\n",
    "#C) Company\n",
    "#D) Skills they hire for\n",
    "#E) Location\n",
    "#Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the link\n",
    "\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on the recruiters button\n",
    "\n",
    "time.sleep(5)\n",
    "button1=driver.find_element_by_xpath(\"//a[@href='https://www.naukri.com/recruiters']\").get_attribute('href')\n",
    "driver.get(button1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering the data science and clicking on search button\n",
    "search_field_designation=driver.find_element_by_xpath('//input[@class=\"sugInp\"]')\n",
    "search_field_designation.send_keys(\"Data science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "search_button=driver.find_element_by_xpath(\"//button[@class='fl qsbSrch blueBtn']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the lists\n",
    "Name=[]\n",
    "Designation=[]\n",
    "Company=[]\n",
    "Skills=[]\n",
    "Location=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracted the name\n",
    "try:\n",
    "\n",
    "    names=driver.find_elements_by_xpath(\"//span[@class='fl ellipsis']\")\n",
    "\n",
    "    for i in names:\n",
    "\n",
    "        Name.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Name.append('No details available') \n",
    "\n",
    "len(Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracted the designation\n",
    "try:\n",
    "\n",
    "    des=driver.find_elements_by_xpath(\"//span[@class='ellipsis clr']\")\n",
    "\n",
    "    for i in des:\n",
    "\n",
    "        Designation.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Designation.append('No details available') \n",
    "\n",
    "len(Designation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "['Data Science Network', 'Shore Infotech India Pvt. Ltd', 'MARSIAN Technologies LLP', 'Enerlytics Software Solutions Pvt Ltd', 'LibraryXProject', 'Apidel Technologies Division of Transpower', 'IFMR', 'Techvantage Systems Pvt Ltd', 'Weupskill- Live Wire India', 'CBL Data Science Private Limited', 'Innominds Software', 'MoneyTap', 'QuantMagnum Technologies Pvt. Ltd.', 'Bristlecone India Ltd', 'SocialPrachar.com', 'BISP Solutions', 'Easi Tax', 'Novelworx Digital Solutions', 'AXESTRACK SOFTWARE SOLUTIONS PRIVATE...', 'FirstTech Consaltants Pvt.Ltd', 'Affine Analytics', 'NEAL ANALYTICS SERVICES PVT LTD', 'Compumatrice Multimedia Pvt Ltd', 'Exela Technologies', 'Autumn Leaf Consulting Services Private...', 'trainin', 'Nanoprecise Sci Corp', 'R.S Consultancy &amp; Services', 'Independent Consultant', 'Confidential', 'Dollarbird Information Services Pvt, Ltd', 'JOBSMILL BUSINESS SOLUTIONS PRIVATE LIMITED', 'Techcovery', 'HyrEzy Talent Solutions LLP', 'Trisect', 'ASCO consulting', 'NY INST', '3D India Staffing Research &amp; Consulting...', 'O.C. Tanner', 'Demand Matrix', 'MADHUSUDHAN SRIDHAR', 'Suntech Global', 'Strategic Consulting Lab', 'Impel Labs Pvt. Ltd.', 'MRP Advisers', 'Saras Solutions India Pvt Ltd', 'WildJasmine', 'LNT Private Limited', 'Granular.ai', 'Certybox Pvt.Ltd.']\n"
     ]
    }
   ],
   "source": [
    "#Extracted the Company\n",
    "try:\n",
    "\n",
    "    com=driver.find_elements_by_xpath(\"//div[@id='tabP-1']/div/div/div/div/p/a[2]\")\n",
    "\n",
    "    for i in com:\n",
    "\n",
    "        Company.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Company.append('No details available') \n",
    "\n",
    "print(len(Company))\n",
    "print(Company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracted the location\n",
    "try:\n",
    "\n",
    "    loc=driver.find_elements_by_xpath(\"//div[@id='tabP-1']/div/div/div/div/p/span/i/small\")\n",
    "\n",
    "    for i in loc:\n",
    "\n",
    "        Location.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Location.append('No details available') \n",
    "\n",
    "len(Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracted the Skills\n",
    "try:\n",
    "\n",
    "    skill=driver.find_elements_by_xpath(\"//div[@class='hireSec highlightable']\")\n",
    "\n",
    "    for i in skill:\n",
    "\n",
    "        Skills.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Skills.append('No details available') \n",
    "\n",
    "len(Skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Designations</th>\n",
       "      <th>Skill</th>\n",
       "      <th>Companies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aakash Harit</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>Classic ASP Developer, Internet Marketing Prof...</td>\n",
       "      <td>Data Science Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shravan Kumar Gaddam</td>\n",
       "      <td>Company Recruiter</td>\n",
       "      <td>.Net, Java, Data Science, Linux Administration...</td>\n",
       "      <td>Shore Infotech India Pvt. Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARSIAN Technologies LLP</td>\n",
       "      <td>Company HR</td>\n",
       "      <td>Data Science, Artificial Intelligence, Machine...</td>\n",
       "      <td>MARSIAN Technologies LLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anik Agrawal</td>\n",
       "      <td>Company Recruiter</td>\n",
       "      <td>Mean Stack, javascript, angularjs, mongodb, We...</td>\n",
       "      <td>Enerlytics Software Solutions Pvt Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subhas patel</td>\n",
       "      <td>Founder CEO</td>\n",
       "      <td>Hadoop, Spark, Digital Strategy, Data Architec...</td>\n",
       "      <td>LibraryXProject</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Names       Designations  \\\n",
       "0              Aakash Harit         HR Manager   \n",
       "1      shravan Kumar Gaddam  Company Recruiter   \n",
       "2  MARSIAN Technologies LLP         Company HR   \n",
       "3              Anik Agrawal  Company Recruiter   \n",
       "4              subhas patel        Founder CEO   \n",
       "\n",
       "                                               Skill  \\\n",
       "0  Classic ASP Developer, Internet Marketing Prof...   \n",
       "1  .Net, Java, Data Science, Linux Administration...   \n",
       "2  Data Science, Artificial Intelligence, Machine...   \n",
       "3  Mean Stack, javascript, angularjs, mongodb, We...   \n",
       "4  Hadoop, Spark, Digital Strategy, Data Architec...   \n",
       "\n",
       "                               Companies  \n",
       "0                   Data Science Network  \n",
       "1          Shore Infotech India Pvt. Ltd  \n",
       "2               MARSIAN Technologies LLP  \n",
       "3  Enerlytics Software Solutions Pvt Ltd  \n",
       "4                        LibraryXProject  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataframe created\n",
    "time.sleep(4)\n",
    "df3=pd.DataFrame({\"Names\":Name,\"Designations\":Designation,\"Skill\":Skills,\"Companies\":Company})\n",
    "df3.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution to question number 8. Scrape the details of Highest selling novels.\n",
    "#Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare/\n",
    "#You have to find the following details:\n",
    "#A) Book name\n",
    "#B) Author name\n",
    "#C) Volumes sold\n",
    "#D) Publisher\n",
    "#E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the link\n",
    "\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "#opening the url\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "driver.get(url)\n",
    "time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "#scrolling the page down\n",
    "driver.execute_script(\"window.scrollTo(0, 2100)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty lists\n",
    "\n",
    "Book_name=[]\n",
    "Author_name=[]\n",
    "Volumes_sold=[]\n",
    "Publisher=[]\n",
    "Genre=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Da Vinci Code,The', 'Harry Potter and the Deathly Hallows', \"Harry Potter and the Philosopher's Stone\", 'Harry Potter and the Order of the Phoenix', 'Fifty Shades of Grey', 'Harry Potter and the Goblet of Fire', 'Harry Potter and the Chamber of Secrets', 'Harry Potter and the Prisoner of Azkaban', 'Angels and Demons', \"Harry Potter and the Half-blood Prince:Children's Edition\", 'Fifty Shades Darker', 'Twilight', 'Girl with the Dragon Tattoo,The:Millennium Trilogy', 'Fifty Shades Freed', 'Lost Symbol,The', 'New Moon', 'Deception Point', 'Eclipse', 'Lovely Bones,The', 'Curious Incident of the Dog in the Night-time,The', 'Digital Fortress', 'Short History of Nearly Everything,A', 'Girl Who Played with Fire,The:Millennium Trilogy', 'Breaking Dawn', 'Very Hungry Caterpillar,The:The Very Hungry Caterpillar', 'Gruffalo,The', \"Jamie's 30-Minute Meals\", 'Kite Runner,The', 'One Day', 'Thousand Splendid Suns,A', \"Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\", \"Time Traveler's Wife,The\", 'Atonement', \"Bridget Jones's Diary:A Novel\", 'World According to Clarkson,The', \"Captain Corelli's Mandolin\", 'Sound of Laughter,The', 'Life of Pi', 'Billy Connolly', 'Child Called It,A', \"Gruffalo's Child,The\", \"Angela's Ashes:A Memoir of a Childhood\", 'Birdsong', 'Northern Lights:His Dark Materials S.', 'Labyrinth', 'Harry Potter and the Half-blood Prince', 'Help,The', 'Man and Boy', 'Memoirs of a Geisha', \"No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\", 'Island,The', 'PS, I Love You', 'You are What You Eat:The Plan That Will Change Your Life', 'Shadow of the Wind,The', 'Tales of Beedle the Bard,The', 'Broker,The', \"Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\", 'Subtle Knife,The:His Dark Materials S.', 'Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation', \"Delia's How to Cook:(Bk.1)\", 'Chocolat', 'Boy in the Striped Pyjamas,The', \"My Sister's Keeper\", 'Amber Spyglass,The:His Dark Materials S.', 'To Kill a Mockingbird', 'Men are from Mars, Women are from Venus:A Practical Guide for Improvin', 'Dear Fatty', 'Short History of Tractors in Ukrainian,A', 'Hannibal', 'Lord of the Rings,The', 'Stupid White Men:...and Other Sorry Excuses for the State of the Natio', 'Interpretation of Murder,The', 'Sharon Osbourne Extreme:My Autobiography', 'Alchemist,The:A Fable About Following Your Dream', \"At My Mother's Knee ...:and Other Low Joints\", 'Notes from a Small Island', 'Return of the Naked Chef,The', 'Bridget Jones: The Edge of Reason', \"Jamie's Italy\", 'I Can Make You Thin', 'Down Under', 'Summons,The', 'Small Island', 'Nigella Express', 'Brick Lane', \"Memory Keeper's Daughter,The\", 'Room on the Broom', 'About a Boy', 'My Booky Wook', 'God Delusion,The', '\"Beano\" Annual,The', 'White Teeth', 'House at Riverton,The', 'Book Thief,The', 'Nights of Rain and Stars', 'Ghost,The', 'Happy Days with the Naked Chef', 'Hunger Games,The:Hunger Games Trilogy', \"Lost Boy,The:A Foster Child's Search for the Love of a Family\", \"Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\"]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Extracted the Book_name\n",
    "try:\n",
    "\n",
    "    book=driver.find_elements_by_xpath(\"//div[@class='embed block']/table/tbody/tr/td[2]\")\n",
    "\n",
    "    for i in book:\n",
    "\n",
    "        Book_name.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Book_name.append('No details available') \n",
    "\n",
    "print(Book_name)\n",
    "print(len(Book_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brown, Dan', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'James, E. L.', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'Brown, Dan', 'Rowling, J.K.', 'James, E. L.', 'Meyer, Stephenie', 'Larsson, Stieg', 'James, E. L.', 'Brown, Dan', 'Meyer, Stephenie', 'Brown, Dan', 'Meyer, Stephenie', 'Sebold, Alice', 'Haddon, Mark', 'Brown, Dan', 'Bryson, Bill', 'Larsson, Stieg', 'Meyer, Stephenie', 'Carle, Eric', 'Donaldson, Julia', 'Oliver, Jamie', 'Hosseini, Khaled', 'Nicholls, David', 'Hosseini, Khaled', 'Larsson, Stieg', 'Niffenegger, Audrey', 'McEwan, Ian', 'Fielding, Helen', 'Clarkson, Jeremy', 'Bernieres, Louis de', 'Kay, Peter', 'Martel, Yann', 'Stephenson, Pamela', 'Pelzer, Dave', 'Donaldson, Julia', 'McCourt, Frank', 'Faulks, Sebastian', 'Pullman, Philip', 'Mosse, Kate', 'Rowling, J.K.', 'Stockett, Kathryn', 'Parsons, Tony', 'Golden, Arthur', 'McCall Smith, Alexander', 'Hislop, Victoria', 'Ahern, Cecelia', 'McKeith, Gillian', 'Zafon, Carlos Ruiz', 'Rowling, J.K.', 'Grisham, John', 'Atkins, Robert C.', 'Pullman, Philip', 'Truss, Lynne', 'Smith, Delia', 'Harris, Joanne', 'Boyne, John', 'Picoult, Jodi', 'Pullman, Philip', 'Lee, Harper', 'Gray, John', 'French, Dawn', 'Lewycka, Marina', 'Harris, Thomas', 'Tolkien, J. R. R.', 'Moore, Michael', 'Rubenfeld, Jed', 'Osbourne, Sharon', 'Coelho, Paulo', \"O'Grady, Paul\", 'Bryson, Bill', 'Oliver, Jamie', 'Fielding, Helen', 'Oliver, Jamie', 'McKenna, Paul', 'Bryson, Bill', 'Grisham, John', 'Levy, Andrea', 'Lawson, Nigella', 'Ali, Monica', 'Edwards, Kim', 'Donaldson, Julia', 'Hornby, Nick', 'Brand, Russell', 'Dawkins, Richard', '0', 'Smith, Zadie', 'Morton, Kate', 'Zusak, Markus', 'Binchy, Maeve', 'Harris, Robert', 'Oliver, Jamie', 'Collins, Suzanne', 'Pelzer, Dave', 'Oliver, Jamie']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Extracted the author name\n",
    "try:\n",
    "\n",
    "    name=driver.find_elements_by_xpath(\"//div[@class='embed block']/table/tbody/tr/td[3]\")\n",
    "\n",
    "    for i in name:\n",
    "\n",
    "        Author_name.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Author_name.append('No details available') \n",
    "\n",
    "print(Author_name)\n",
    "print(len(Author_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5,094,805', '4,475,152', '4,200,654', '4,179,479', '3,758,936', '3,583,215', '3,484,047', '3,377,906', '3,193,946', '2,950,264', '2,479,784', '2,315,405', '2,233,570', '2,193,928', '2,183,031', '2,152,737', '2,062,145', '2,052,876', '2,005,598', '1,979,552', '1,928,900', '1,852,919', '1,814,784', '1,787,118', '1,783,535', '1,781,269', '1,743,266', '1,629,119', '1,616,068', '1,583,992', '1,555,135', '1,546,886', '1,539,428', '1,508,205', '1,489,403', '1,352,318', '1,310,207', '1,310,176', '1,231,957', '1,217,712', '1,208,711', '1,204,058', '1,184,967', '1,181,503', '1,181,093', '1,153,181', '1,132,336', '1,130,802', '1,126,337', '1,115,549', '1,108,328', '1,107,379', '1,104,403', '1,092,349', '1,090,847', '1,087,262', '1,054,196', '1,037,160', '1,023,688', '1,015,956', '1,009,873', '1,004,414', '1,003,780', '1,002,314', '998,213', '992,846', '986,753', '986,115', '970,509', '967,466', '963,353', '962,515', '959,496', '956,114', '945,640', '931,312', '925,425', '924,695', '906,968', '905,086', '890,847', '869,671', '869,659', '862,602', '856,540', '845,858', '842,535', '828,215', '820,563', '816,907', '816,585', '815,586', '814,370', '809,641', '808,900', '807,311', '794,201', '792,187', '791,507', '791,095']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Extracted the volumne sold\n",
    "try:\n",
    "\n",
    "    vol=driver.find_elements_by_xpath(\"//div[@class='embed block']/table/tbody/tr/td[4]\")\n",
    "\n",
    "    for i in vol:\n",
    "\n",
    "        Volumes_sold.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Volumes_sold.append('No details available') \n",
    "\n",
    "print(Volumes_sold)\n",
    "print(len(Volumes_sold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Transworld', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Random House', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Transworld', 'Bloomsbury', 'Random House', 'Little, Brown Book', 'Quercus', 'Random House', 'Transworld', 'Little, Brown Book', 'Transworld', 'Little, Brown Book', 'Pan Macmillan', 'Random House', 'Transworld', 'Transworld', 'Quercus', 'Little, Brown Book', 'Penguin', 'Pan Macmillan', 'Penguin', 'Bloomsbury', 'Hodder & Stoughton', 'Bloomsbury', 'Quercus', 'Random House', 'Random House', 'Pan Macmillan', 'Penguin', 'Random House', 'Random House', 'Canongate', 'HarperCollins', 'Orion', 'Pan Macmillan', 'HarperCollins', 'Random House', 'Scholastic Ltd.', 'Orion', 'Bloomsbury', 'Penguin', 'HarperCollins', 'Random House', 'Little, Brown Book', 'Headline', 'HarperCollins', 'Penguin', 'Orion', 'Bloomsbury', 'Random House', 'Random House', 'Scholastic Ltd.', 'Profile Books Group', 'Random House', 'Transworld', 'Random House Childrens Books G', 'Hodder & Stoughton', 'Scholastic Ltd.', 'Random House', 'HarperCollins', 'Random House', 'Penguin', 'Random House', 'HarperCollins', 'Penguin', 'Headline', 'Little, Brown Book', 'HarperCollins', 'Transworld', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Transworld', 'Transworld', 'Random House', 'Headline', 'Random House', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Hodder & Stoughton', 'Transworld', 'D.C. Thomson', 'Penguin', 'Pan Macmillan', 'Transworld', 'Orion', 'Random House', 'Penguin', 'Scholastic Ltd.', 'Orion', 'Penguin']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Extracted the publisher\n",
    "try:\n",
    "\n",
    "    pub=driver.find_elements_by_xpath(\"//div[@class='embed block']/table/tbody/tr/td[5]\")\n",
    "\n",
    "    for i in pub:\n",
    "\n",
    "        Publisher.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Publisher.append('No details available') \n",
    "\n",
    "print(Publisher)\n",
    "print(len(Publisher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crime, Thriller & Adventure', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Romance & Sagas', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Crime, Thriller & Adventure', \"Children's Fiction\", 'Romance & Sagas', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Romance & Sagas', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Popular Science', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Picture Books', 'Picture Books', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Humour: Collections & General', 'General & Literary Fiction', 'Autobiography: General', 'General & Literary Fiction', 'Biography: The Arts', 'Autobiography: General', 'Picture Books', 'Autobiography: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Science Fiction & Fantasy', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'Fitness & Diet', 'General & Literary Fiction', \"Children's Fiction\", 'Crime, Thriller & Adventure', 'Fitness & Diet', 'Young Adult Fiction', 'Usage & Writing Guides', 'Food & Drink: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Popular Culture & Media: General Interest', 'Autobiography: The Arts', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Science Fiction & Fantasy', 'Current Affairs & Issues', 'Crime, Thriller & Adventure', 'Autobiography: The Arts', 'General & Literary Fiction', 'Autobiography: The Arts', 'Travel Writing', 'Food & Drink: General', 'General & Literary Fiction', 'National & Regional Cuisine', 'Fitness & Diet', 'Travel Writing', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'Picture Books', 'General & Literary Fiction', 'Autobiography: The Arts', 'Popular Science', \"Children's Annuals\", 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Food & Drink: General', 'Young Adult Fiction', 'Biography: General', 'Food & Drink: General']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Extracted the publisher\n",
    "try:\n",
    "\n",
    "    gen=driver.find_elements_by_xpath(\"//div[@class='embed block']/table/tbody/tr/td[6]\")\n",
    "\n",
    "    for i in gen:\n",
    "\n",
    "        Genre.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Genre.append('No details available') \n",
    "\n",
    "print(Genre)\n",
    "print(len(Genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book_names</th>\n",
       "      <th>Author_names</th>\n",
       "      <th>Volumes_solds</th>\n",
       "      <th>Publishers</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Book_names   Author_names Volumes_solds  \\\n",
       "0                          Da Vinci Code,The     Brown, Dan     5,094,805   \n",
       "1       Harry Potter and the Deathly Hallows  Rowling, J.K.     4,475,152   \n",
       "2   Harry Potter and the Philosopher's Stone  Rowling, J.K.     4,200,654   \n",
       "3  Harry Potter and the Order of the Phoenix  Rowling, J.K.     4,179,479   \n",
       "4                       Fifty Shades of Grey   James, E. L.     3,758,936   \n",
       "\n",
       "     Publishers                       Genres  \n",
       "0    Transworld  Crime, Thriller & Adventure  \n",
       "1    Bloomsbury           Children's Fiction  \n",
       "2    Bloomsbury           Children's Fiction  \n",
       "3    Bloomsbury           Children's Fiction  \n",
       "4  Random House              Romance & Sagas  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataframe created\n",
    "time.sleep(4)\n",
    "df3=pd.DataFrame({\"Book_names\":Book_name,\"Author_names\":Author_name,\"Volumes_solds\":Volumes_sold,\"Publishers\":Publisher,\"Genres\":Genre})\n",
    "df3.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution to Q 9. Scrape the details most watched tv series of all time from imdb.com.\n",
    "#Url = https://www.imdb.com/list/ls095964455/\n",
    "#You have to find the following details:\n",
    "#A) Name\n",
    "#B) Year span\n",
    "#C) Genre\n",
    "#D) Run time\n",
    "#E) Ratings\n",
    "#F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the link\n",
    "\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "#opening the url\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "\n",
    "Name=[]\n",
    "Year_span=[]\n",
    "Genr=[]\n",
    "Run_time=[]\n",
    "Ratings=[]\n",
    "Votes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Game of Thrones', 'Stranger Things', 'The Walking Dead', '13 Reasons Why', 'The 100', 'Orange Is the New Black', 'Riverdale', \"Grey's Anatomy\", 'The Flash', 'Arrow', 'Money Heist', 'The Big Bang Theory', 'Black Mirror', 'Sherlock', 'Vikings', 'Pretty Little Liars', 'The Vampire Diaries', 'American Horror Story', 'Breaking Bad', 'Lucifer', 'Supernatural', 'Prison Break', 'How to Get Away with Murder', 'Teen Wolf', 'The Simpsons', 'Once Upon a Time', 'Narcos', 'Daredevil', 'Friends', 'How I Met Your Mother', 'Suits', 'Mr. Robot', 'The Originals', 'Supergirl', 'Gossip Girl', 'Sense8', 'Gotham', 'Westworld', 'Jessica Jones', 'Modern Family', 'Rick and Morty', 'Shadowhunters', 'The End of the F***ing World', 'House of Cards', 'Dark', 'Elite', 'Sex Education', 'Shameless', 'New Girl', 'Agents of S.H.I.E.L.D.', 'You', 'Dexter', 'Fear the Walking Dead', 'Family Guy', 'The Blacklist', 'Lost', 'Peaky Blinders', 'House', 'Quantico', 'Orphan Black', 'Homeland', 'Blindspot', \"DC's Legends of Tomorrow\", \"The Handmaid's Tale\", 'Chilling Adventures of Sabrina', 'The Good Doctor', 'Jane the Virgin', 'Glee', 'South Park', 'Brooklyn Nine-Nine', 'Under the Dome', 'The Umbrella Academy', 'True Detective', 'The OA', 'Desperate Housewives', 'Better Call Saul', 'Bates Motel', 'The Punisher', 'Atypical', 'Dynasty', 'This Is Us', 'The Good Place', 'Iron Fist', 'The Rain', 'Mindhunter', 'Revenge', 'Luke Cage', 'Scandal', 'The Defenders', 'Big Little Lies', 'Insatiable', 'The Mentalist', 'The Crown', 'Chernobyl', 'iZombie', 'Reign', 'A Series of Unfortunate Events', 'Criminal Minds', 'Scream: The TV Series', 'The Haunting of Hill House']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Extracted the name\n",
    "try:\n",
    "\n",
    "    name=driver.find_elements_by_xpath(\"//div[@class='lister-item mode-detail']/div[2]/h3/a\")\n",
    "\n",
    "    for i in name:\n",
    "\n",
    "        Name.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Name.append('No details available') \n",
    "\n",
    "print(Name)\n",
    "print(len(Name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(2011–2019)', '(2016– )', '(2010–2022)', '(2017–2020)', '(2014–2020)', '(2013–2019)', '(2017– )', '(2005– )', '(2014– )', '(2012–2020)', '(2017–2021)', '(2007–2019)', '(2011– )', '(2010–2017)', '(2013–2020)', '(2010–2017)', '(2009–2017)', '(2011– )', '(2008–2013)', '(2016– )', '(2005–2020)', '(2005–2017)', '(2014–2020)', '(2011–2017)', '(1989– )', '(2011–2018)', '(2015–2017)', '(2015–2018)', '(1994–2004)', '(2005–2014)', '(2011–2019)', '(2015–2019)', '(2013–2018)', '(2015–2021)', '(2007–2012)', '(2015–2018)', '(2014–2019)', '(2016– )', '(2015–2019)', '(2009–2020)', '(2013– )', '(2016–2019)', '(2017–2019)', '(2013–2018)', '(2017–2020)', '(2018– )', '(2019– )', '(2011–2021)', '(2011–2018)', '(2013–2020)', '(2018– )', '(2006–2021)', '(2015– )', '(1999–2022)', '(2013– )', '(2004–2010)', '(2013– )', '(2004–2012)', '(2015–2018)', '(2013–2017)', '(2011–2020)', '(2015–2020)', '(2016– )', '(2017– )', '(2018–2020)', '(2017– )', '(2014–2019)', '(2009–2015)', '(1997– )', '(2013–2022)', '(2013–2015)', '(2019– )', '(2014–2019)', '(2016–2019)', '(2004–2012)', '(2015– )', '(2013–2017)', '(2017– )', '(2017–2021)', '(2017– )', '(2016–2022)', '(2016–2020)', '(2017–2018)', '(2018–2020)', '(2017–2019)', '(2011–2015)', '(2016–2018)', '(2012–2018)', '(2017)', '(2017–2019)', '(2018–2019)', '(2008–2015)', '(2016– )', '(2019)', '(2015–2019)', '(2013–2017)', '(2017–2019)', '(2005–2020)', '(2015–2019)', '(2018)']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Extracted the year span\n",
    "try:\n",
    "\n",
    "    year=driver.find_elements_by_xpath(\"//div[@class='lister-item mode-detail']/div[2]/h3/span[2]\")\n",
    "\n",
    "    for i in year:\n",
    "\n",
    "        Year_span.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Year_span.append('No details available') \n",
    "\n",
    "print(Year_span)\n",
    "print(len(Year_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removeing square brackets from list\n",
    "# using str() + list slicing\n",
    "\n",
    "#for i in Year_span:\n",
    "#    Year_span_withoutbrackets = str(i)[1:-1]\n",
    "\n",
    "\n",
    "#Year_span_withoutbrackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action, Adventure, Drama', 'Drama, Fantasy, Horror', 'Drama, Horror, Thriller', 'Drama, Mystery, Thriller', 'Drama, Mystery, Sci-Fi', 'Comedy, Crime, Drama', 'Crime, Drama, Mystery', 'Drama, Romance', 'Action, Adventure, Drama', 'Action, Adventure, Crime', 'Action, Crime, Mystery', 'Comedy, Romance', 'Drama, Sci-Fi, Thriller', 'Crime, Drama, Mystery', 'Action, Adventure, Drama', 'Drama, Mystery, Romance', 'Drama, Fantasy, Horror', 'Drama, Horror, Thriller', 'Crime, Drama, Thriller', 'Crime, Drama, Fantasy', 'Drama, Fantasy, Horror', 'Action, Crime, Drama', 'Crime, Drama, Mystery', 'Action, Drama, Fantasy', 'Animation, Comedy', 'Adventure, Fantasy, Romance', 'Biography, Crime, Drama', 'Action, Crime, Drama', 'Comedy, Romance', 'Comedy, Romance', 'Comedy, Drama', 'Crime, Drama, Thriller', 'Drama, Fantasy, Horror', 'Action, Adventure, Drama', 'Drama, Romance', 'Drama, Mystery, Sci-Fi', 'Action, Crime, Drama', 'Drama, Mystery, Sci-Fi', 'Action, Crime, Drama', 'Comedy, Drama, Romance', 'Animation, Adventure, Comedy', 'Action, Drama, Fantasy', 'Adventure, Comedy, Crime', 'Drama', 'Crime, Drama, Mystery', 'Crime, Drama, Thriller', 'Comedy, Drama', 'Comedy, Drama', 'Comedy', 'Action, Adventure, Drama', 'Crime, Drama, Romance', 'Crime, Drama, Mystery', 'Drama, Horror, Sci-Fi', 'Animation, Comedy', 'Crime, Drama, Mystery', 'Adventure, Drama, Fantasy', 'Crime, Drama', 'Drama, Mystery', 'Crime, Drama, Mystery', 'Action, Drama, Sci-Fi', 'Crime, Drama, Mystery', 'Action, Crime, Drama', 'Action, Adventure, Drama', 'Drama, Sci-Fi, Thriller', 'Drama, Fantasy, Horror', 'Drama', 'Comedy', 'Comedy, Drama, Music', 'Animation, Comedy', 'Comedy, Crime', 'Drama, Mystery, Sci-Fi', 'Action, Adventure, Comedy', 'Crime, Drama, Mystery', 'Drama, Fantasy, Mystery', 'Comedy, Drama, Mystery', 'Crime, Drama', 'Drama, Horror, Mystery', 'Action, Crime, Drama', 'Comedy, Drama', 'Drama', 'Comedy, Drama, Romance', 'Comedy, Drama, Fantasy', 'Action, Adventure, Crime', 'Drama, Sci-Fi, Thriller', 'Crime, Drama, Thriller', 'Drama, Mystery, Thriller', 'Action, Crime, Drama', 'Drama, Thriller', 'Action, Adventure, Crime', 'Crime, Drama, Mystery', 'Comedy, Drama, Thriller', 'Crime, Drama, Mystery', 'Biography, Drama, History', 'Drama, History, Thriller', 'Comedy, Crime, Drama', 'Drama, Fantasy', 'Adventure, Comedy, Drama', 'Crime, Drama, Mystery', 'Comedy, Crime, Drama', 'Drama, Horror, Mystery']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Extracted the genre\n",
    "try:\n",
    "\n",
    "    ge=driver.find_elements_by_xpath(\"//div[@class='lister-item mode-detail']/div[2]/p/span[5]\")\n",
    "\n",
    "    for i in ge:\n",
    "\n",
    "        Genr.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Genr.append('No details available') \n",
    "\n",
    "print(Genr)\n",
    "print(len(Genr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['57 min', '51 min', '44 min', '60 min', '43 min', '59 min', '45 min', '41 min', '43 min', '42 min', '70 min', '22 min', '60 min', '88 min', '44 min', '44 min', '43 min', '60 min', '49 min', '42 min', '44 min', '44 min', '43 min', '41 min', '22 min', '60 min', '49 min', '54 min', '22 min', '22 min', '44 min', '49 min', '45 min', '43 min', '42 min', '60 min', '42 min', '62 min', '56 min', '22 min', '23 min', '42 min', '25 min', '51 min', '60 min', '60 min', '45 min', '46 min', '22 min', '45 min', '45 min', '53 min', '44 min', '22 min', '43 min', '44 min', '60 min', '44 min', '42 min', '44 min', '55 min', '42 min', '42 min', '60 min', '60 min', '41 min', '60 min', '44 min', '22 min', '22 min', '43 min', '60 min', '55 min', '60 min', '45 min', '46 min', '45 min', '53 min', '30 min', '42 min', '45 min', '22 min', '55 min', '45 min', '60 min', '44 min', '55 min', '43 min', '50 min', '60 min', '45 min', '43 min', '58 min', '330 min', '42 min', '42 min', '50 min', '42 min', '45 min', '572 min']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Extracted the Run_time\n",
    "try:\n",
    "\n",
    "    run=driver.find_elements_by_xpath(\"//div[@class='lister-item mode-detail']/div[2]/p/span[3]\")\n",
    "\n",
    "    for i in run:\n",
    "\n",
    "        Run_time.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Run_time.append('No details available') \n",
    "\n",
    "print(Run_time)\n",
    "print(len(Run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.imdb.com/title/tt0944947/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4574334/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1520211/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1837492/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2661044/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2372162/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt5420376/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0413573/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt3107288/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2193021/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt6468322/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0898266/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2085059/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1475582/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2306299/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1578873/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1405406/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1844624/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0903747/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4052886/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0460681/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0455275/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt3205802/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1567432/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0096697/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1843230/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2707408/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt3322312/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0108778/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0460649/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1632701/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4158110/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2632424/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4016454/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0397442/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2431438/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt3749900/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0475784/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2357547/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1442437/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2861424/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4145054/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt6257970/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1856010/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt5753856/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt7134908/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt7767422/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1586680/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1826940/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2364582/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt7335184/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0773262/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt3743822/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0182576/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2741602/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0411008/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2442560/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0412142/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4428122/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2234222/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1796960/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4474344/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4532368/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt5834204/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt7569592/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt6470478/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt3566726/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1327801/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0121955/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2467372/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1553656/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1312171/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2356777/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4635282/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0410975/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt3032476/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2188671/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt5675620/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt6315640/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt6128300/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt5555260/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4955642/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt3322310/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt6656238/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt5290382/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1837642/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt3322314/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1837576/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4230076/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt3920596/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt6487482/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt1196946/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4786824/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt7366338/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt3501584/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt2710394/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt4834206/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt0452046/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt3921180/?ref_=ttls_li_tt', 'https://www.imdb.com/title/tt6763664/?ref_=ttls_li_tt']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "page_urls=[]\n",
    "time.sleep(3)\n",
    "# scrape next pages urls\n",
    "nxt_page = driver.find_elements_by_xpath(\"//h3[@class='lister-item-header']/a\")\n",
    "for i in nxt_page:\n",
    "    page_urls.append(i.get_attribute('href'))\n",
    "    \n",
    "\n",
    "print(page_urls)\n",
    "print(len(page_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_urls\n",
    "\n",
    "#for url in page_urls:\n",
    "#    driver.get(url)\n",
    "\n",
    "#Extracted the Ratings\n",
    "#try:\n",
    "\n",
    "#    rat=driver.find_elements_by_xpath(\"//span[@class='AggregateRatingButton__RatingScore-sc-1ll29m0-1 iTLWoV']\")\n",
    "\n",
    "#    for i in rat:\n",
    "\n",
    "#       Ratings.append(i.text)\n",
    "\n",
    "#except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "#    Ratings.append('No details available') \n",
    "\n",
    "#print(Ratings)\n",
    "#print(len(Ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,835,393', '874,161', '879,319', '264,212', '225,709', '283,523', '125,076', '263,661', '316,948', '413,424', '337,414', '740,421', '466,004', '834,669', '455,512', '155,225', '292,045', '283,971', '1,539,328', '258,797', '401,760', '490,116', '135,168', '132,021', '373,713', '211,935', '373,001', '372,850', '872,451', '621,130', '372,686', '343,913', '121,865', '114,749', '158,527', '143,418', '215,562', '442,084', '197,094', '374,090', '404,718', '55,779', '156,104', '475,022', '309,268', '57,930', '174,475', '212,104', '199,286', '204,284', '155,754', '661,238', '117,302', '311,122', '210,844', '509,685', '386,735', '424,317', '57,992', '103,905', '321,245', '67,978', '95,166', '191,958', '81,899', '68,554', '41,207', '139,333', '338,457', '245,512', '102,393', '173,337', '512,923', '94,109', '119,184', '344,466', '99,765', '193,013', '66,365', '16,400', '115,454', '125,759', '118,098', '32,583', '233,119', '114,469', '119,541', '68,465', '93,919', '166,664', '23,880', '169,475', '167,935', '589,235', '61,902', '44,790', '55,397', '169,584', '35,120', '193,351']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Extracted the votes\n",
    "try:\n",
    "\n",
    "    vot=driver.find_elements_by_xpath(\"//div[@class='lister-item mode-detail']/div[2]/p[4]/span[2]\")\n",
    "\n",
    "    for i in vot:\n",
    "\n",
    "        Votes.append(i.text)\n",
    "\n",
    "except NoSuchElementException:        #handling no such element exception\n",
    "\n",
    "    Votes.append('No details available') \n",
    "\n",
    "print(Votes)\n",
    "print(len(Votes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Years_span</th>\n",
       "      <th>Genrr</th>\n",
       "      <th>Run_times</th>\n",
       "      <th>Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>1,835,393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016– )</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>874,161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>879,319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>264,212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>225,709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Names   Years_span                     Genrr Run_times  \\\n",
       "0   Game of Thrones  (2011–2019)  Action, Adventure, Drama    57 min   \n",
       "1   Stranger Things     (2016– )    Drama, Fantasy, Horror    51 min   \n",
       "2  The Walking Dead  (2010–2022)   Drama, Horror, Thriller    44 min   \n",
       "3    13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller    60 min   \n",
       "4           The 100  (2014–2020)    Drama, Mystery, Sci-Fi    43 min   \n",
       "\n",
       "        Vote  \n",
       "0  1,835,393  \n",
       "1    874,161  \n",
       "2    879,319  \n",
       "3    264,212  \n",
       "4    225,709  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataframe created\n",
    "time.sleep(4)\n",
    "df4=pd.DataFrame({\"Names\":Name,\"Years_span\":Year_span,\"Genrr\":Genr,\"Run_times\":Run_time,\"Vote\":Votes})\n",
    "df4.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution to Q.10. Details of Datasets from UCI machine learning repositories.\n",
    "#Url = https://archive.ics.uci.edu/\n",
    "#You have to find the following details:\n",
    "#A) Dataset name\n",
    "#B) Data type\n",
    "#C) Task\n",
    "#D) Attribute type\n",
    "#E) No of instances\n",
    "#F) No of attribute\n",
    "#G) Year\n",
    "#Note: - from the home page you have to go to the Show All Dataset page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Importing selenium webdriver \n",
    "from selenium import webdriver\n",
    "\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException,ElementNotInteractableException\n",
    "\n",
    "#Importing requests\n",
    "import requests\n",
    "\n",
    "# importing regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# Opening the imdb.com\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "#clicking show all datasets link\n",
    "show=driver.find_element_by_xpath(\"//a[@href='datasets.php']\")\n",
    "show.click()\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "#scraping dataset names\n",
    "name=[]\n",
    "try:\n",
    "    names=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[1]/table/tbody/tr/td[2]/p/b/a\")\n",
    "    for i in names:\n",
    "        name.append(i.text)\n",
    "except NoSuchElementException:#handling no such element exception\n",
    "    name.append('No details available')\n",
    "time.sleep(2)\n",
    "\n",
    "#scraping data type\n",
    "data_type=[]\n",
    "try:\n",
    "    info=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]\")\n",
    "    for i in info[1:]:\n",
    "         data_type.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    data_type.append(\"No information\")\n",
    "    \n",
    "#scraping task\n",
    "Task=[]\n",
    "try:\n",
    "    info1=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[3]\")\n",
    "    for i in info1[1:]:\n",
    "         Task.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Task.append(\"No information\")\n",
    "\n",
    "#scraping attribute type\n",
    "Attribute_type=[]\n",
    "\n",
    "try:\n",
    "    info2=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]\")\n",
    "    for i in info2[1:]:\n",
    "         Attribute_type.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Attribute_type.append(\"No information\")\n",
    "\n",
    "\n",
    "#scraping number of instances\n",
    "No_of_instances=[]\n",
    "\n",
    "try:\n",
    "    info3=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[5]\")\n",
    "    for i in info3[1:]:\n",
    "         No_of_instances.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    No_of_instances.append(\"No information\")\n",
    "    \n",
    "#scraping number of attribute\n",
    "No_of_attribute=[]\n",
    "\n",
    "try:\n",
    "    info4=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[6]\")\n",
    "    for i in info4[1:]:\n",
    "         No_of_attribute.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    No_of_attribute.append(\"No information\")\n",
    "    \n",
    "\n",
    "#scraping year\n",
    "Year=[]\n",
    "\n",
    "try:\n",
    "    info5=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[7]\")\n",
    "    for i in info5[1:]:\n",
    "         Year.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Year.append(\"No information\")\n",
    "    \n",
    "len(Year)\n",
    "len(No_of_attribute)\n",
    "len(No_of_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Data_Types</th>\n",
       "      <th>Tasks</th>\n",
       "      <th>Attributes_type</th>\n",
       "      <th>No_of_instance</th>\n",
       "      <th>Years</th>\n",
       "      <th>No_of_attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abalone</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>4177</td>\n",
       "      <td>1995</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48842</td>\n",
       "      <td>1996</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annealing</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>798</td>\n",
       "      <td></td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anonymous Microsoft Web Data</td>\n",
       "      <td></td>\n",
       "      <td>Recommender-Systems</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>37711</td>\n",
       "      <td>1998</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arrhythmia</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>452</td>\n",
       "      <td>1998</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Names     Data_Types                 Tasks  \\\n",
       "0                       Abalone  Multivariate        Classification    \n",
       "1                         Adult  Multivariate        Classification    \n",
       "2                     Annealing  Multivariate        Classification    \n",
       "3  Anonymous Microsoft Web Data                 Recommender-Systems    \n",
       "4                    Arrhythmia  Multivariate        Classification    \n",
       "\n",
       "               Attributes_type No_of_instance  Years No_of_attributes  \n",
       "0  Categorical, Integer, Real           4177   1995                8   \n",
       "1        Categorical, Integer          48842   1996               14   \n",
       "2  Categorical, Integer, Real            798                      38   \n",
       "3                 Categorical          37711   1998              294   \n",
       "4  Categorical, Integer, Real            452   1998              279   "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataframe created\n",
    "time.sleep(4)\n",
    "df5=pd.DataFrame({\"Names\":name,\"Data_Types\":data_type,\"Tasks\":Task,\"Attributes_type\":Attribute_type,\"No_of_instance\":No_of_instances,\"Years\":Year,\"No_of_attributes\":No_of_attribute})\n",
    "df5.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
